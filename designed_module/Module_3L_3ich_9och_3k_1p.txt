nn.Conv2d( 3, 9, 3, padding=1)
nn.Conv2d( 9, 9, 3, padding=1)
nn.Conv2d( 9, 1, 3, padding=1)
******************************
[Model_txt_log_path] = C:\Users\alibh\Desktop\My_Qt\Paper_2_PyQT_Pytorch\designed_module\Module_3L_3ich_9och_3k_1p.txt
******************************
[Model_name] = Module_3L_3ich_9och_3k_1p
[Model_code] = 0003_003L
******************************

Epoch 0/9
----------
LR 0.0001
train: loss: 0.863511, bce: 0.739667, dice: 0.987355
val: loss: 0.861924, bce: 0.736622, dice: 0.987226
0m 1s
Epoch 1/9
----------
LR 0.0001
train: loss: 0.861052, bce: 0.734815, dice: 0.987290
val: loss: 0.859479, bce: 0.731799, dice: 0.987158
0m 1s
Epoch 2/9
----------
LR 0.0001
train: loss: 0.858615, bce: 0.730009, dice: 0.987222
val: loss: 0.857056, bce: 0.727021, dice: 0.987090
0m 1s
Epoch 3/9
----------
LR 0.0001
train: loss: 0.856201, bce: 0.725247, dice: 0.987155
val: loss: 0.854653, bce: 0.722285, dice: 0.987021
0m 1s
Epoch 4/9
----------
LR 0.0001
train: loss: 0.853805, bce: 0.720524, dice: 0.987086
val: loss: 0.852267, bce: 0.717583, dice: 0.986951
0m 1s
Epoch 5/9
----------
LR 0.0001
train: loss: 0.851424, bce: 0.715833, dice: 0.987015
val: loss: 0.849892, bce: 0.712906, dice: 0.986878
0m 1s
Epoch 6/9
----------
LR 0.0001
train: loss: 0.849051, bce: 0.711161, dice: 0.986941
val: loss: 0.847521, bce: 0.708239, dice: 0.986802
0m 1s
Epoch 7/9
----------
LR 0.0001
train: loss: 0.846680, bce: 0.706493, dice: 0.986867
val: loss: 0.845145, bce: 0.703568, dice: 0.986722
0m 1s
Epoch 8/9
----------
LR 0.0001
train: loss: 0.844300, bce: 0.701815, dice: 0.986785
val: loss: 0.842757, bce: 0.698876, dice: 0.986638
0m 1s
Epoch 9/9
----------
LR 0.0001
train: loss: 0.841906, bce: 0.697111, dice: 0.986701
val: loss: 0.840348, bce: 0.694148, dice: 0.986549
0m 1s
******************************
Best val loss: 0.840348 at 9 epoch.
******************************
The loss is in this format [sample number, epoch number , binary_cross_entropy_with_logits , defined_loss , total loss, learning rate]
[loss] ={'loss_bce_Val': [(0, 0, 0.7366224527359009, 0.9872255325317383, 0.8619239926338196, 0.0001), (20, 1, 0.7317992448806763, 0.987157940864563, 0.8594785928726196, 0.0001), (40, 2, 0.7270211577415466, 0.9870901107788086, 0.8570556640625, 0.0001), (60, 3, 0.7222849130630493, 0.987021267414093, 0.8546531200408936, 0.0001), (80, 4, 0.7175832390785217, 0.986950695514679, 0.8522669672966003, 0.0001), (100, 5, 0.7129057049751282, 0.9868779182434082, 0.8498917818069458, 0.0001), (120, 6, 0.7082390189170837, 0.9868021011352539, 0.8475205898284912, 0.0001), (140, 7, 0.703567624092102, 0.9867224097251892, 0.8451449871063232, 0.0001), (160, 8, 0.6988759636878967, 0.9866384863853455, 0.8427572250366211, 0.0001), (180, 9, 0.6941481232643127, 0.986548900604248, 0.840348482131958, 0.0001)], 'loss_bce_train': [(0, 0, 0.7414835691452026, 0.9872093200683594, 0.864346444606781, 0.0001), (50, 0, 0.7409267425537109, 0.9879291951656342, 0.8644279539585114, 0.0001), (100, 0, 0.7402975161870321, 0.9876588980356852, 0.8639782071113586, 0.0001), (150, 0, 0.7396669685840607, 0.9873551577329636, 0.8635110706090927, 0.0001), (200, 1, 0.7365919351577759, 0.9868173003196716, 0.8617045879364014, 0.0001), (250, 1, 0.736004501581192, 0.9870549440383911, 0.8615297079086304, 0.0001), (300, 1, 0.7353982329368591, 0.9870142539342245, 0.8612062335014343, 0.0001), (350, 1, 0.7348149418830872, 0.9872896522283554, 0.8610522896051407, 0.0001), (400, 2, 0.7317861914634705, 0.987009584903717, 0.8593978881835938, 0.0001), (450, 2, 0.7312258183956146, 0.987546980381012, 0.8593863844871521, 0.0001), (500, 2, 0.7306047876675924, 0.9872031013170878, 0.8589039246241251, 0.0001), (550, 2, 0.7300090789794922, 0.987221896648407, 0.8586154729127884, 0.0001), (600, 3, 0.7269999384880066, 0.9867836833000183, 0.8568918108940125, 0.0001), (650, 3, 0.726444661617279, 0.9873594343662262, 0.8569020330905914, 0.0001), (700, 3, 0.7258374293645223, 0.9871387879053751, 0.8564880887667338, 0.0001), (750, 3, 0.7252469509840012, 0.9871546477079391, 0.856200784444809, 0.0001), (800, 4, 0.7222765684127808, 0.9868993163108826, 0.8545879125595093, 0.0001), (850, 4, 0.7217061221599579, 0.9871878325939178, 0.8544469475746155, 0.0001), (900, 4, 0.7210940718650818, 0.9868082602818807, 0.8539511362711588, 0.0001), (950, 4, 0.7205241620540619, 0.9870862066745758, 0.8538051545619965, 0.0001), (1000, 5, 0.7175936102867126, 0.9871214032173157, 0.8523575067520142, 0.0001), (1050, 5, 0.7170082330703735, 0.9871179461479187, 0.8520630896091461, 0.0001), (1100, 5, 0.716431736946106, 0.987255314985911, 0.851843535900116, 0.0001), (1150, 5, 0.7158327847719193, 0.9870148599147797, 0.85142382979393, 0.0001), (1200, 6, 0.7129721641540527, 0.988017737865448, 0.8504949808120728, 0.0001), (1250, 6, 0.7123477756977081, 0.987318217754364, 0.8498330116271973, 0.0001), (1300, 6, 0.7117467125256857, 0.9870036840438843, 0.8493752082188925, 0.0001), (1350, 6, 0.7111605405807495, 0.9869413673877716, 0.8490509688854218, 0.0001), (1400, 7, 0.7081938982009888, 0.986017107963562, 0.8471055030822754, 0.0001), (1450, 7, 0.7076168656349182, 0.9861250817775726, 0.8468709588050842, 0.0001), (1500, 7, 0.7070701122283936, 0.9867640932401022, 0.8469170928001404, 0.0001), (1550, 7, 0.7064927518367767, 0.986866757273674, 0.8466797471046448, 0.0001), (1600, 8, 0.7035806775093079, 0.9869627356529236, 0.8452717065811157, 0.0001), (1650, 8, 0.7029953896999359, 0.9869504570960999, 0.8449729084968567, 0.0001), (1700, 8, 0.7024199565251669, 0.9871304233868917, 0.8447751800219218, 0.0001), (1750, 8, 0.7018154412508011, 0.9867845177650452, 0.8442999720573425, 0.0001), (1800, 9, 0.6988815665245056, 0.9867531657218933, 0.8428173661231995, 0.0001), (1850, 9, 0.6982671618461609, 0.986259251832962, 0.8422631919384003, 0.0001), (1900, 9, 0.6976820429166158, 0.986335833867391, 0.8420089284578959, 0.0001), (1950, 9, 0.6971111297607422, 0.9867009967565536, 0.8419060558080673, 0.0001)]}
******************************
