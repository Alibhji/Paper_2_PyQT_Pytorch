nn.Conv2d( 3, 5, 3, padding=1)
nn.Conv2d( 5, 5, 3, padding=1)
nn.Conv2d( 5, 6, 3, padding=1)
******************************
[Model_txt_log_path] = C:\Users\alibh\Desktop\My_Qt\Paper_2_PyQT_Pytorch\designed_module\Module_3L_3ich_5och_3k_1p.txt
******************************
[Model_name] = Module_3L_3ich_5och_3k_1p
[Model_code] = 0001_003L
******************************

Epoch 0/9
----------
LR 0.0001
train: dice: 0.990619, bce: 0.707045, loss: 0.848832
val: dice: 0.990268, bce: 0.706484, loss: 0.848376
0m 1s
Epoch 1/9
----------
LR 0.0001
train: dice: 0.990616, bce: 0.706147, loss: 0.848382
val: dice: 0.990263, bce: 0.705606, loss: 0.847935
0m 1s
Epoch 2/9
----------
LR 0.0001
train: dice: 0.990612, bce: 0.705281, loss: 0.847946
val: dice: 0.990258, bce: 0.704757, loss: 0.847508
0m 1s
Epoch 3/9
----------
LR 0.0001
train: dice: 0.990606, bce: 0.704441, loss: 0.847524
val: dice: 0.990252, bce: 0.703930, loss: 0.847091
0m 1s
Epoch 4/9
----------
LR 0.0001
train: dice: 0.990601, bce: 0.703620, loss: 0.847110
val: dice: 0.990246, bce: 0.703115, loss: 0.846681
0m 2s
Epoch 5/9
----------
LR 0.0001
train: dice: 0.990596, bce: 0.702806, loss: 0.846701
val: dice: 0.990240, bce: 0.702302, loss: 0.846271
0m 2s
Epoch 6/9
----------
LR 0.0001
train: dice: 0.990591, bce: 0.701991, loss: 0.846291
val: dice: 0.990235, bce: 0.701483, loss: 0.845859
0m 1s
Epoch 7/9
----------
LR 0.0001
train: dice: 0.990587, bce: 0.701166, loss: 0.845876
val: dice: 0.990231, bce: 0.700648, loss: 0.845439
0m 1s
Epoch 8/9
----------
LR 0.0001
train: dice: 0.990583, bce: 0.700323, loss: 0.845453
val: dice: 0.990227, bce: 0.699792, loss: 0.845009
0m 1s
Epoch 9/9
----------
LR 0.0001
train: dice: 0.990579, bce: 0.699456, loss: 0.845018
val: dice: 0.990224, bce: 0.698907, loss: 0.844565
0m 1s
******************************
Best val loss: 0.844565 at 9 epoch.
******************************
The loss is in this format [sample number, epoch number , binary_cross_entropy_with_logits , defined_loss , total loss, learning rate]
[loss] ={'loss_bce_Val': [(0, 0, 0.7064841985702515, 0.9902681112289429, 0.8483761548995972, 0.0001), (20, 1, 0.7056060433387756, 0.9902634620666504, 0.8479347229003906, 0.0001), (40, 2, 0.7047571539878845, 0.990257978439331, 0.8475075960159302, 0.0001), (60, 3, 0.7039299607276917, 0.9902519583702087, 0.8470909595489502, 0.0001), (80, 4, 0.7031151056289673, 0.9902458786964417, 0.8466805219650269, 0.0001), (100, 5, 0.7023023962974548, 0.9902402758598328, 0.8462713360786438, 0.0001), (120, 6, 0.7014827132225037, 0.9902352690696716, 0.8458589911460876, 0.0001), (140, 7, 0.7006480693817139, 0.9902306795120239, 0.8454393744468689, 0.0001), (160, 8, 0.6997915506362915, 0.9902269840240479, 0.8450092673301697, 0.0001), (180, 9, 0.6989067792892456, 0.9902240037918091, 0.8445653915405273, 0.0001)], 'loss_bce_train': [(0, 0, 0.7073956727981567, 0.9906664490699768, 0.8490310907363892, 0.0001), (50, 0, 0.7072729170322418, 0.9906558096408844, 0.8489643931388855, 0.0001), (100, 0, 0.7071574330329895, 0.9905990958213806, 0.8488782842954, 0.0001), (150, 0, 0.7070453613996506, 0.9906194657087326, 0.8488324284553528, 0.0001), (200, 1, 0.7064999938011169, 0.9908928871154785, 0.8486964702606201, 0.0001), (250, 1, 0.7063730657100677, 0.9907388091087341, 0.8485559523105621, 0.0001), (300, 1, 0.7062607010205587, 0.9907425244649252, 0.8485016226768494, 0.0001), (350, 1, 0.7061474621295929, 0.9906159192323685, 0.8483816981315613, 0.0001), (400, 2, 0.7055957913398743, 0.9904747009277344, 0.8480352163314819, 0.0001), (450, 2, 0.7054906189441681, 0.9906237721443176, 0.8480571806430817, 0.0001), (500, 2, 0.7053896188735962, 0.9907044172286987, 0.8480469981829325, 0.0001), (550, 2, 0.7052810192108154, 0.9906115233898163, 0.8479462563991547, 0.0001), (600, 3, 0.7047529816627502, 0.9906787276268005, 0.8477158546447754, 0.0001), (650, 3, 0.7046496272087097, 0.9905728995800018, 0.8476112484931946, 0.0001), (700, 3, 0.7045462926228842, 0.990591565767924, 0.8475689093271891, 0.0001), (750, 3, 0.7044411301612854, 0.9906064420938492, 0.8475237786769867, 0.0001), (800, 4, 0.7039280533790588, 0.9904354214668274, 0.8471817374229431, 0.0001), (850, 4, 0.7038234174251556, 0.99029141664505, 0.847057431936264, 0.0001), (900, 4, 0.7037251989046732, 0.9904897212982178, 0.847107470035553, 0.0001), (950, 4, 0.703619509935379, 0.9906012117862701, 0.8471103757619858, 0.0001), (1000, 5, 0.7031052708625793, 0.9906434416770935, 0.8468743562698364, 0.0001), (1050, 5, 0.7030112147331238, 0.9907172322273254, 0.8468642234802246, 0.0001), (1100, 5, 0.7029096881548563, 0.9906399448712667, 0.8467748165130615, 0.0001), (1150, 5, 0.7028059661388397, 0.9905957728624344, 0.8467008769512177, 0.0001), (1200, 6, 0.702296257019043, 0.9910306930541992, 0.8466634750366211, 0.0001), (1250, 6, 0.7021957039833069, 0.9907287359237671, 0.846462219953537, 0.0001), (1300, 6, 0.7020906011263529, 0.9905980428059896, 0.8463443319002787, 0.0001), (1350, 6, 0.7019910216331482, 0.9905908554792404, 0.8462909460067749, 0.0001), (1400, 7, 0.7014911770820618, 0.9905251264572144, 0.8460081815719604, 0.0001), (1450, 7, 0.7013813257217407, 0.9905699491500854, 0.8459756374359131, 0.0001), (1500, 7, 0.7012701829274496, 0.990537961324056, 0.8459040721257528, 0.0001), (1550, 7, 0.7011657804250717, 0.9905865788459778, 0.8458761870861053, 0.0001), (1600, 8, 0.7006490230560303, 0.990688681602478, 0.8456688523292542, 0.0001), (1650, 8, 0.7005455493927002, 0.9907017052173615, 0.845623642206192, 0.0001), (1700, 8, 0.7004332741101583, 0.9906261563301086, 0.8455297350883484, 0.0001), (1750, 8, 0.7003229111433029, 0.9905826449394226, 0.8454528003931046, 0.0001), (1800, 9, 0.6997698545455933, 0.9905750751495361, 0.8451724648475647, 0.0001), (1850, 9, 0.6996675431728363, 0.9904854595661163, 0.8450765013694763, 0.0001), (1900, 9, 0.6995681524276733, 0.9906203548113505, 0.8450942436854044, 0.0001), (1950, 9, 0.6994556188583374, 0.9905794560909271, 0.8450175225734711, 0.0001)]}
******************************
