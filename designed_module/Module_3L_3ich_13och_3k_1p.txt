nn.Conv2d( 3, 13, 3, padding=1)
nn.Conv2d( 13, 13, 3, padding=1)
nn.Conv2d( 13, 6, 3, padding=1)
******************************
[Model_txt_log_path] = /home/ali/Alibhji/Paper_2_PyQT_Pytorch/designed_module/Module_3L_3ich_13och_3k_1p.txt
******************************
[Model_name] = Module_3L_3ich_13och_3k_1p
[Model_code] = 0005_003L
******************************

Epoch 0/9
----------
LR 0.0001
train: bce: 0.669560, dice: 0.990360, loss: 0.829960
val: bce: 0.667554, dice: 0.990067, loss: 0.828811
0m 0s
Epoch 1/9
----------
LR 0.0001
train: bce: 0.666325, dice: 0.990406, loss: 0.828366
val: bce: 0.664270, dice: 0.990111, loss: 0.827191
0m 0s
Epoch 2/9
----------
LR 0.0001
train: bce: 0.663005, dice: 0.990450, loss: 0.826728
val: bce: 0.660878, dice: 0.990156, loss: 0.825517
0m 0s
Epoch 3/9
----------
LR 0.0001
train: bce: 0.659563, dice: 0.990496, loss: 0.825029
val: bce: 0.657338, dice: 0.990205, loss: 0.823771
0m 0s
Epoch 4/9
----------
LR 0.0001
train: bce: 0.655961, dice: 0.990545, loss: 0.823253
val: bce: 0.653618, dice: 0.990259, loss: 0.821938
0m 0s
Epoch 5/9
----------
LR 0.0001
train: bce: 0.652164, dice: 0.990602, loss: 0.821383
val: bce: 0.649681, dice: 0.990322, loss: 0.820001
0m 0s
Epoch 6/9
----------
LR 0.0001
train: bce: 0.648138, dice: 0.990666, loss: 0.819402
val: bce: 0.645495, dice: 0.990394, loss: 0.817944
0m 0s
Epoch 7/9
----------
LR 0.0001
train: bce: 0.643852, dice: 0.990741, loss: 0.817297
val: bce: 0.641027, dice: 0.990478, loss: 0.815753
0m 0s
Epoch 8/9
----------
LR 0.0001
train: bce: 0.639273, dice: 0.990828, loss: 0.815051
val: bce: 0.636249, dice: 0.990575, loss: 0.813412
0m 0s
Epoch 9/9
----------
LR 0.0001
train: bce: 0.634375, dice: 0.990928, loss: 0.812651
val: bce: 0.631137, dice: 0.990686, loss: 0.810912
0m 0s
******************************
Best val loss: 0.810912 at 9 epoch.
******************************
The loss is in this format [sample number, epoch number , binary_cross_entropy_with_logits , defined_loss , total loss, learning rate]
[loss] ={'loss_bce_train': [(0, 0, 0.6707477569580078, 0.9903080463409424, 0.8305279016494751, 0.0001), (50, 0, 0.6703545153141022, 0.9903354048728943, 0.8303449749946594, 0.0001), (100, 0, 0.6699596246083578, 0.9903414050738016, 0.8301505247751871, 0.0001), (150, 0, 0.669559583067894, 0.9903604537248611, 0.8299600183963776, 0.0001), (200, 1, 0.6675483584403992, 0.9905477166175842, 0.8290480375289917, 0.0001), (250, 1, 0.6671412885189056, 0.9904953241348267, 0.8288183212280273, 0.0001), (300, 1, 0.6667323708534241, 0.9904107848803202, 0.828571597735087, 0.0001), (350, 1, 0.6663252711296082, 0.9904062151908875, 0.828365758061409, 0.0001), (400, 2, 0.6642762422561646, 0.9907666444778442, 0.8275214433670044, 0.0001), (450, 2, 0.6638503074645996, 0.9905211329460144, 0.827185720205307, 0.0001), (500, 2, 0.6634289820988973, 0.9904429912567139, 0.8269359866778055, 0.0001), (550, 2, 0.6630053818225861, 0.9904500246047974, 0.8267277032136917, 0.0001), (600, 3, 0.6608790159225464, 0.9906837940216064, 0.8257814049720764, 0.0001), (650, 3, 0.6604454815387726, 0.9905539155006409, 0.8254996836185455, 0.0001), (700, 3, 0.6600009004275004, 0.9904612104098002, 0.8252310554186503, 0.0001), (750, 3, 0.6595631390810013, 0.9904956519603729, 0.8250293880701065, 0.0001), (800, 4, 0.657340407371521, 0.9905916452407837, 0.8239660263061523, 0.0001), (850, 4, 0.6568948328495026, 0.9905659258365631, 0.8237303793430328, 0.0001), (900, 4, 0.6564305623372396, 0.9905852874120077, 0.8235079248746237, 0.0001), (950, 4, 0.6559605598449707, 0.9905453771352768, 0.8232529610395432, 0.0001), (1000, 5, 0.6536219716072083, 0.9907953143119812, 0.8222086429595947, 0.0001), (1050, 5, 0.6531440019607544, 0.9907486140727997, 0.8219463229179382, 0.0001), (1100, 5, 0.6526554226875305, 0.9905793070793152, 0.8216173648834229, 0.0001), (1150, 5, 0.6521635353565216, 0.9906020760536194, 0.8213828057050705, 0.0001), (1200, 6, 0.6496911644935608, 0.99066561460495, 0.8201783895492554, 0.0001), (1250, 6, 0.6491827368736267, 0.9906831681728363, 0.8199329376220703, 0.0001), (1300, 6, 0.6486686070760092, 0.9906713167826334, 0.8196699619293213, 0.0001), (1350, 6, 0.6481380611658096, 0.990666463971138, 0.8194022625684738, 0.0001), (1400, 7, 0.6454850435256958, 0.9904014468193054, 0.8179432153701782, 0.0001), (1450, 7, 0.6449529826641083, 0.9906289875507355, 0.8177909851074219, 0.0001), (1500, 7, 0.6444073518117269, 0.9906598726908366, 0.8175336122512817, 0.0001), (1550, 7, 0.6438523232936859, 0.9907411336898804, 0.8172967284917831, 0.0001), (1600, 8, 0.6410317420959473, 0.9909428358078003, 0.8159872889518738, 0.0001), (1650, 8, 0.6404615640640259, 0.9908918738365173, 0.8156767189502716, 0.0001), (1700, 8, 0.6398683587710062, 0.9908283551534017, 0.8153483668963114, 0.0001), (1750, 8, 0.639272928237915, 0.99082812666893, 0.8150505274534225, 0.0001), (1800, 9, 0.6362473964691162, 0.990858793258667, 0.8135530948638916, 0.0001), (1850, 9, 0.6356250643730164, 0.9908531010150909, 0.8132390975952148, 0.0001), (1900, 9, 0.6350024739901224, 0.9908533692359924, 0.8129279216130575, 0.0001), (1950, 9, 0.6343748420476913, 0.990927666425705, 0.8126512467861176, 0.0001)], 'loss_bce_Val': [(0, 0, 0.6675541400909424, 0.9900668859481812, 0.8288105130195618, 0.0001), (20, 1, 0.6642703413963318, 0.990111231803894, 0.8271907567977905, 0.0001), (40, 2, 0.6608775854110718, 0.9901561141014099, 0.8255168199539185, 0.0001), (60, 3, 0.6573382616043091, 0.9902046918869019, 0.8237714767456055, 0.0001), (80, 4, 0.6536176800727844, 0.9902589917182922, 0.8219383358955383, 0.0001), (100, 5, 0.6496806740760803, 0.9903216361999512, 0.8200011253356934, 0.0001), (120, 6, 0.6454946994781494, 0.990393877029419, 0.8179442882537842, 0.0001), (140, 7, 0.6410271525382996, 0.9904779195785522, 0.8157525062561035, 0.0001), (160, 8, 0.6362493634223938, 0.9905748963356018, 0.8134121298789978, 0.0001), (180, 9, 0.6311370730400085, 0.9906860589981079, 0.8109115362167358, 0.0001)]}
******************************
